<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>phc.easy.query API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>.flex{display:flex !important}body{line-height:1.5em;font-family:"Lato",Helvetica,Roboto,Arial,"Lucida Grande",sans-serif}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:90%}ul li code a{font-size:80%}a{color:#00539a;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#f7944d}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<link rel="shortcut icon" href="./favicon.ico">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>phc.easy.query</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import math
from typing import Any, Callable, List, Optional, Tuple, Union

import pandas as pd
from phc.base_client import BaseClient
from phc.easy.auth import Auth
from phc.easy.query.api_paging import clean_params, recursive_paging_api_call
from phc.easy.query.fhir_aggregation import FhirAggregation
from phc.easy.query.fhir_dsl import (DEFAULT_SCROLL_SIZE, MAX_RESULT_SIZE,
                                     execute_single_fhir_dsl,
                                     recursive_execute_fhir_dsl, tqdm,
                                     with_progress)
from phc.easy.query.fhir_dsl_query import build_query
from phc.easy.query.ga4gh import recursive_execute_ga4gh
from phc.easy.util import extract_codes
from phc.easy.util.api_cache import FHIR_DSL, APICache
from phc.services import Fhir
from toolz import identity


class Query:
    @staticmethod
    def find_count_of_dsl_query(query: dict, auth_args: Auth = Auth.shared()):
        &#34;&#34;&#34;Find count of a given dsl query

        See https://docs.us.lifeomic.com/development/fhir-service/dsl/

        Attributes
        ----------
        query : dict
            The FHIR query to run a count against

        auth_args : Auth, dict
            Additional arguments for authentication

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.find_count_of_dsl_query({
          &#34;type&#34;: &#34;select&#34;,
          &#34;columns&#34;: &#34;*&#34;,
          &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
        })
        &#34;&#34;&#34;
        if FhirAggregation.is_aggregation_query(query):
            raise ValueError(&#34;Count is not support for aggregation queries.&#34;)

        auth = Auth(auth_args)
        fhir = Fhir(auth.session())

        response = fhir.execute_es(
            auth.project_id, build_query(query, page_size=1), scroll=&#34;true&#34;
        )

        return response.data[&#34;hits&#34;][&#34;total&#34;][&#34;value&#34;]

    @staticmethod
    def execute_fhir_dsl(
        query: dict,
        all_results: bool = False,
        auth_args: Auth = Auth.shared(),
        callback: Union[Callable[[Any, bool], None], None] = None,
        max_pages: Union[int, None] = None,
        log: bool = False,
        **query_kwargs,
    ):
        &#34;&#34;&#34;Execute a FHIR query with the DSL

        See https://docs.us.lifeomic.com/development/fhir-service/dsl/

        Attributes
        ----------
        query : dict
            The FHIR query to run (is a superset of elasticsearch)

        all_results : bool
            Return all results by scrolling through mutliple pages of data
            (Limit is ignored if provided)

        auth_args : Auth, dict
            Additional arguments for authentication

        callback : Callable[[Any, bool], None] (optional)
            A progress function that is invoked for each batch. When the second
            argument passed is true, then the result of the callback function is
            used as the return value. This is useful if writing results out to a
            file and then returning the completed result from that file.

            Example:

                def handle_batch(batch, is_finished):
                    print(len(batch))
                    if is_finished:
                        return &#34;batch finished

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_fhir_dsl({
          &#34;type&#34;: &#34;select&#34;,
          &#34;columns&#34;: &#34;*&#34;,
          &#34;from&#34;: [
              {&#34;table&#34;: &#34;patient&#34;}
          ],
        }, all_results=True)

        &#34;&#34;&#34;
        query = build_query(query, **query_kwargs)

        if log:
            print(json.dumps(query, indent=4))

        if FhirAggregation.is_aggregation_query(query):
            response = execute_single_fhir_dsl(query, auth_args=auth_args)
            return FhirAggregation.from_response(response)

        if all_results:
            return with_progress(
                lambda: tqdm(total=MAX_RESULT_SIZE),
                lambda progress: recursive_execute_fhir_dsl(
                    {
                        &#34;limit&#34;: [
                            {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: 0},
                            # Make window size smaller than maximum to reduce
                            # pressure on API
                            {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: DEFAULT_SCROLL_SIZE},
                        ],
                        **query,
                    },
                    scroll=all_results,
                    progress=progress,
                    callback=callback,
                    auth_args=auth_args,
                    max_pages=max_pages,
                ),
            )

        return recursive_execute_fhir_dsl(
            query,
            scroll=all_results,
            callback=callback,
            auth_args=auth_args,
            max_pages=max_pages,
        )

    @staticmethod
    def execute_paging_api(
        path: str,
        params: dict = {},
        http_verb: str = &#34;GET&#34;,
        transform: Callable[[pd.DataFrame], pd.DataFrame] = identity,
        all_results: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Optional[int] = None,
        page_size: Optional[int] = None,
        log: bool = False,
        raw: bool = False,
        ignore_cache: bool = False,
        show_progress: bool = True,
        progress: Optional[tqdm] = None,
        item_key: str = &#34;items&#34;,
        try_count: bool = True,
    ):
        &#34;&#34;&#34;Execute a API query that pages through results

        See https://docs.us.lifeomic.com/api/?shell#lifeomic-core-api-genomics
        for example

        Attributes
        ----------
        path : str
            The API path to hit
            (Special tokens: `:project_id`)

        params : dict
            The parameters to include with request

        http_verb : str
            The HTTP method to use

        all_results : bool = False
            Retrieve sample of results (25) or entire set of records

        auth_args : Auth, dict
            Additional arguments for authentication

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        page_size : int
            The number of records to fetch per page

        log : bool = False
            Whether to log some diagnostic statements for debugging

        progress : Optional[tqdm] = None
            Override the given progress indicator

        item_key : str
            The key to find the results underneath (usually &#34;items&#34; but not always)

        try_count : bool
            Whether to try and send a &#34;count&#34; param to update the progress bar

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_paging_api(
                &#34;genomics/projects/:project_id/tests&#34;,
                params={
                    &#34;patientId&#34;: &#34;&lt;patient-uuid&gt;&#34;
                }
            )

        &#34;&#34;&#34;

        auth = Auth(auth_args)

        params = clean_params(params)

        # Do not pull project_id if not in URL (which throws error if project not selected)
        if &#34;project_id&#34; in path:
            path = path.replace(&#34;:project_id&#34;, auth.project_id)

        query = {&#34;path&#34;: path, &#34;method&#34;: http_verb, &#34;params&#34;: params}

        if all_results and page_size is None:
            # Default to 100 if not provided but getting all results
            page_size = 100

        if log:
            print(json.dumps(query, indent=4))

        use_cache = (
            (not ignore_cache)
            and (not raw)
            and all_results
            and (max_pages is None)
        )

        if use_cache and APICache.does_cache_for_query_exist(query):
            return APICache.load_cache_for_query(query)

        callback = (
            APICache.build_cache_callback(query, transform, nested_key=None)
            if use_cache
            else None
        )

        results = with_progress(
            lambda: (progress if progress is not None else tqdm())
            if show_progress
            else None,
            lambda progress: recursive_paging_api_call(
                path,
                params=params,
                http_verb=http_verb,
                callback=callback,
                scroll=all_results or (max_pages is not None),
                max_pages=max_pages,
                page_size=page_size,
                log=log,
                auth_args=auth_args,
                progress=progress,
                item_key=item_key,
                try_count=try_count,
            ),
        )

        df = pd.DataFrame(results)

        if raw:
            return df

        return transform(df)

    @staticmethod
    def execute_fhir_dsl_with_options(
        query: dict,
        transform: Callable[[pd.DataFrame], pd.DataFrame],
        all_results: bool,
        raw: bool,
        query_overrides: dict,
        auth_args: Auth,
        ignore_cache: bool,
        max_pages: Union[int, None],
        log: bool = False,
        **query_kwargs,
    ):
        query = build_query({**query, **query_overrides}, **query_kwargs)

        if log:
            print(json.dumps(query, indent=4))

        use_cache = (
            (not ignore_cache)
            and (not raw)
            and (all_results or FhirAggregation.is_aggregation_query(query))
            and (max_pages is None)
        )

        if use_cache and APICache.does_cache_for_query_exist(
            query, namespace=FHIR_DSL
        ):
            return APICache.load_cache_for_query(query, namespace=FHIR_DSL)

        callback = (
            APICache.build_cache_callback(query, transform, namespace=FHIR_DSL)
            if use_cache
            else None
        )

        results = Query.execute_fhir_dsl(
            query,
            all_results,
            auth_args,
            callback=callback,
            max_pages=max_pages,
        )

        if isinstance(results, FhirAggregation):
            # Cache isn&#39;t written in batches so we need to explicitly do it here
            if use_cache:
                APICache.write_agg(query, results)

            return results

        if isinstance(results, pd.DataFrame):
            return results

        df = pd.DataFrame(map(lambda r: r[&#34;_source&#34;], results))

        if raw:
            return df

        return transform(df)

    @staticmethod
    def get_codes(
        table_name: str,
        code_fields: List[str],
        display_query: Optional[str] = None,
        sample_size: Optional[int] = None,
        **kwargs,
    ):
        &#34;&#34;&#34;Find FHIR codes with a display for a given table

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        code_fields : List[str]
            The fields of this table that contain a system, code, and display

        display_query : Optional[str]
            Part of the code&#39;s display to match (will try to extract full code
            if passed)

        sample_size : Optional[int]
            Override the search size for finding codes (may miss codes on later
            records)

        kwargs : dict
            Arguments to pass to `phc.easy.query.Query.execute_composite_aggregations`

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.get_codes(
            table_name=&#34;observation&#34;,
            code_fields=[&#34;meta.tag&#34;, &#34;code.coding&#34;],
            patient_id=&#34;&lt;my-patient-id&gt;&#34;
        )
        &#34;&#34;&#34;
        if len(code_fields) == 0:
            raise ValueError(&#34;No code columns specified.&#34;)

        def agg_composite_to_frame(prefix: str, data: dict):
            frame = pd.json_normalize(data[&#34;buckets&#34;])
            frame.columns = frame.columns.str.lstrip(&#34;key.&#34;)
            frame[&#34;field&#34;] = prefix
            return frame

        if display_query is not None:
            kwargs = {
                **kwargs,
                &#34;query_overrides&#34;: {
                    &#34;where&#34;: {
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;query&#34;: {
                            &#34;multi_match&#34;: {
                                &#34;query&#34;: display_query,
                                &#34;fields&#34;: [
                                    f&#34;{key}.display&#34; for key in code_fields
                                ],
                            }
                        },
                    }
                },
            }

        results = Query.execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=[
                (
                    field,
                    [
                        {
                            &#34;display&#34;: {
                                &#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.display.keyword&#34;}
                            }
                        }
                    ],
                )
                for field in code_fields
            ],
            **kwargs,
        )

        agg_result = (
            pd.concat(
                [
                    agg_composite_to_frame(key, value)
                    for key, value in results.items()
                ]
            )
            .pipe(
                lambda df: df
                if len(df) == 0 or display_query is None
                # Poor man&#39;s way to filter only matching codes (since Elasticsearch
                # returns records which will include other codes)
                else df[
                    df[&#34;display&#34;]
                    .str.lower()
                    .str.contains(display_query.lower())
                ]
            )
            .pipe(
                lambda df: pd.DataFrame()
                if len(df) == 0
                else df.sort_values(&#34;doc_count&#34;, ascending=False).reset_index(
                    drop=True
                )
            )
        )

        if display_query is None or len(agg_result) == 0:
            return agg_result

        min_count = sample_size or agg_result.doc_count.sum()
        filtered_code_fields = agg_result.field.unique()

        # Shortcut: If one result, we just need to get the other associated
        # attributes of the code
        if len(agg_result) == 1:
            min_count = 1

        code_results = Query.execute_fhir_dsl(
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;from&#34;: [{&#34;table&#34;: table_name}],
                &#34;columns&#34;: [
                    {
                        &#34;expr&#34;: {
                            &#34;type&#34;: &#34;column_ref&#34;,
                            &#34;column&#34;: key.split(&#34;.&#34;)[0],
                        }
                    }
                    for key in filtered_code_fields
                ],
                &#34;where&#34;: {
                    &#34;type&#34;: &#34;elasticsearch&#34;,
                    &#34;query&#34;: {
                        &#34;multi_match&#34;: {
                            &#34;query&#34;: display_query,
                            &#34;fields&#34;: [
                                f&#34;{key}.display&#34; for key in filtered_code_fields
                            ],
                        }
                    },
                },
            },
            page_size=int(min_count % 9000),
            max_pages=int(math.ceil(min_count / 9000)),
            log=kwargs.get(&#34;log&#34;, False),
        )

        codes = extract_codes(
            map(lambda d: d[&#34;_source&#34;], code_results),
            display_query,
            code_fields,
        )

        if len(codes) == 0:
            return codes

        if len(codes) == codes.display.nunique():
            # If display values are unique, then the counts from Elasticsearch
            # are correct. We can therefore join them.
            codes = (
                codes.join(
                    agg_result[[&#34;display&#34;, &#34;doc_count&#34;]].set_index(&#34;display&#34;),
                    on=&#34;display&#34;,
                    how=&#34;outer&#34;,
                )
                .sort_values(&#34;doc_count&#34;, ascending=False)
                .reset_index(drop=True)
            )

            if len(codes[codes.field.isnull()]) &gt; 0:
                print(
                    &#34;Records with missing system/code values were not retrieved.&#34;
                )

            return codes

        return codes

    @staticmethod
    def execute_composite_aggregations(
        table_name: str,
        key_sources_pairs: List[Tuple[str, List[dict]]],
        batch_size: int = 100,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Union[int, None] = None,
        **query_kwargs,
    ):
        &#34;&#34;&#34;Count records by multiple fields

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        key_sources_pairs : str
            Pairs of keys and sources to pull composite results from

            Example Input:
                [
                    (&#34;meta.tag&#34;, [{&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.system.keyword&#34;}}])
                ]

        batch_size : int
            The size of each page from elasticsearch to use

        query_overrides : dict
            Parts of the FSS query to override
            (Note that passing certain values can cause the method to error out)

            Example aggregation query executed (can use log=True to inspect):
                {
                    &#34;type&#34;: &#34;select&#34;,
                    &#34;columns&#34;: [{
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            &#34;results&#34;: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: [{
                                        &#34;meta.tag&#34;: {
                                            &#34;terms&#34;: {
                                                &#34;field&#34;: &#34;meta.tag.system.keyword&#34;
                                            }
                                        }
                                    }],
                                    &#34;size&#34;: 100,
                                }
                            }
                        },
                    }],
                    &#34;from&#34;: [{&#34;table&#34;: &#34;observation&#34;}],
                }


        auth_args : Auth, dict
            Additional arguments for authentication

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. See :func:`~phc.easy.query.fhir_dsl_query.build_query`.

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_composite_aggregations(
            table_name=&#34;observation&#34;,
            key_sources_pairs=[
                (&#34;meta.tag&#34;, [
                    {&#34;code&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.code.keyword&#34;}}},
                ]),
                (&#34;code.coding&#34;, [
                    {&#34;display&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;code.coding.display.keyword&#34;}}}
                ]),
            ]
        )
        &#34;&#34;&#34;
        if len(key_sources_pairs) == 0:
            raise ValueError(&#34;No aggregate composite terms specified.&#34;)

        return with_progress(
            tqdm,
            lambda progress: Query._recursive_execute_composite_aggregations(
                table_name=table_name,
                key_sources_pairs=key_sources_pairs,
                batch_size=batch_size,
                progress=progress,
                log=log,
                auth_args=auth_args,
                query_overrides=query_overrides,
                max_pages=max_pages,
                **query_kwargs,
            ),
        )

    @staticmethod
    def get_count_by_field(
        table_name: str,
        field: str,
        batch_size: int = 1000,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        **query_kwargs,
    ):
        &#34;&#34;&#34;Count records by a given field

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        field : str
            The field name to count the values of (e.g. &#34;subject.reference&#34;)

        batch_size : int
            The size of each page from elasticsearch to use

        query_overrides : dict
            Parts of the FSS query to override
            (Note that passing certain values can cause the method to error out)

            The aggregation query is similar to this:
                {
                    &#34;type&#34;: &#34;select&#34;,
                    &#34;columns&#34;: [{
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            &#34;results&#34;: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: [{
                                        &#34;value&#34;: {
                                            &#34;terms&#34;: {
                                                &#34;field&#34;: &#34;gender.keyword&#34;
                                            }
                                        }
                                    }],
                                    &#34;size&#34;: 100,
                                }
                            }
                        },
                    }],
                    &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
                }


        auth_args : Auth, dict
            Additional arguments for authentication

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.get_count_by_field(
            table_name=&#34;patient&#34;,
            field=&#34;gender&#34;
        )
        &#34;&#34;&#34;
        data = Query.execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=[
                (
                    &#34;results&#34;,
                    [{&#34;value&#34;: {&#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.keyword&#34;}}}],
                )
            ],
            batch_size=batch_size,
            log=log,
            auth_args=auth_args,
            query_overrides=query_overrides,
            **query_kwargs,
        )

        return pd.DataFrame(
            [
                {field: r[&#34;key&#34;][&#34;value&#34;], &#34;doc_count&#34;: r[&#34;doc_count&#34;]}
                for r in data[&#34;results&#34;][&#34;buckets&#34;]
            ]
        )

    @staticmethod
    def execute_ga4gh(
        query: dict, all_results: bool = False, auth_args: dict = Auth.shared()
    ) -&gt; pd.DataFrame:
        auth = Auth(auth_args)
        client = BaseClient(auth.session())
        path = query[&#34;path&#34;]
        http_verb = query.get(&#34;http_verb&#34;, &#34;POST&#34;)
        results_key = query[&#34;results_key&#34;]
        params = {
            **{&#34;datasetIds&#34;: [auth.project_id]},
            **{
                k: v for k, v in query.items() if k not in [&#34;path&#34;, &#34;http_verb&#34;]
            },
        }

        return recursive_execute_ga4gh(
            auth=auth,
            client=client,
            path=path,
            http_verb=http_verb,
            results_key=results_key,
            params=params,
            scroll=all_results,
        )

    @staticmethod
    def _recursive_execute_composite_aggregations(
        table_name: str,
        key_sources_pairs: List[Tuple[str, List[dict]]],
        batch_size: int = 100,
        progress: Union[tqdm, None] = None,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Union[int, None] = None,
        _current_page: int = 1,
        _prev_results: dict = {},
        _after_keys: dict = {},
        **query_kwargs,
    ):
        aggregation = Query.execute_fhir_dsl(
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;columns&#34;: [
                    {
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            key: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: sources,
                                    &#34;size&#34;: batch_size,
                                    **(
                                        {&#34;after&#34;: _after_keys[key]}
                                        if key in _after_keys
                                        else {}
                                    ),
                                }
                            }
                            for key, sources in key_sources_pairs
                            if (len(_after_keys) == 0) or (key in _after_keys)
                        },
                    }
                ],
                &#34;from&#34;: [{&#34;table&#34;: table_name}],
                **query_overrides,
            },
            auth_args=auth_args,
            log=log,
            **query_kwargs,
        )

        current_results = aggregation.data
        results = FhirAggregation.reduce_composite_results(
            _prev_results, current_results
        )

        if (progress is not None) and (_current_page == 1) and max_pages:
            progress.reset(max_pages)

        if progress is not None:
            # Update by count or pages (if max_pages specified)
            progress.update(
                1
                if max_pages
                else FhirAggregation.count_composite_results(current_results)
            )

        after_keys = FhirAggregation.find_composite_after_keys(
            current_results, batch_size
        )

        if len(after_keys) == 0 or (
            (max_pages is not None) and (_current_page &gt;= max_pages)
        ):
            print(
                f&#34;Retrieved {FhirAggregation.count_composite_results(results)} results&#34;
            )
            return results

        return Query._recursive_execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=key_sources_pairs,
            batch_size=batch_size,
            progress=progress,
            query_overrides=query_overrides,
            log=log,
            auth_args=auth_args,
            max_pages=max_pages,
            _current_page=_current_page + 1,
            _prev_results=results,
            _after_keys=after_keys,
            **query_kwargs,
        )</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="phc.easy.query.api_paging" href="api_paging.html">phc.easy.query.api_paging</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="phc.easy.query.fhir_aggregation" href="fhir_aggregation.html">phc.easy.query.fhir_aggregation</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="phc.easy.query.fhir_dsl" href="fhir_dsl.html">phc.easy.query.fhir_dsl</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="phc.easy.query.fhir_dsl_query" href="fhir_dsl_query.html">phc.easy.query.fhir_dsl_query</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="phc.easy.query.ga4gh" href="ga4gh.html">phc.easy.query.ga4gh</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="phc.easy.query.Query"><code class="flex name class">
<span>class <span class="ident">Query</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Query:
    @staticmethod
    def find_count_of_dsl_query(query: dict, auth_args: Auth = Auth.shared()):
        &#34;&#34;&#34;Find count of a given dsl query

        See https://docs.us.lifeomic.com/development/fhir-service/dsl/

        Attributes
        ----------
        query : dict
            The FHIR query to run a count against

        auth_args : Auth, dict
            Additional arguments for authentication

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.find_count_of_dsl_query({
          &#34;type&#34;: &#34;select&#34;,
          &#34;columns&#34;: &#34;*&#34;,
          &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
        })
        &#34;&#34;&#34;
        if FhirAggregation.is_aggregation_query(query):
            raise ValueError(&#34;Count is not support for aggregation queries.&#34;)

        auth = Auth(auth_args)
        fhir = Fhir(auth.session())

        response = fhir.execute_es(
            auth.project_id, build_query(query, page_size=1), scroll=&#34;true&#34;
        )

        return response.data[&#34;hits&#34;][&#34;total&#34;][&#34;value&#34;]

    @staticmethod
    def execute_fhir_dsl(
        query: dict,
        all_results: bool = False,
        auth_args: Auth = Auth.shared(),
        callback: Union[Callable[[Any, bool], None], None] = None,
        max_pages: Union[int, None] = None,
        log: bool = False,
        **query_kwargs,
    ):
        &#34;&#34;&#34;Execute a FHIR query with the DSL

        See https://docs.us.lifeomic.com/development/fhir-service/dsl/

        Attributes
        ----------
        query : dict
            The FHIR query to run (is a superset of elasticsearch)

        all_results : bool
            Return all results by scrolling through mutliple pages of data
            (Limit is ignored if provided)

        auth_args : Auth, dict
            Additional arguments for authentication

        callback : Callable[[Any, bool], None] (optional)
            A progress function that is invoked for each batch. When the second
            argument passed is true, then the result of the callback function is
            used as the return value. This is useful if writing results out to a
            file and then returning the completed result from that file.

            Example:

                def handle_batch(batch, is_finished):
                    print(len(batch))
                    if is_finished:
                        return &#34;batch finished

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_fhir_dsl({
          &#34;type&#34;: &#34;select&#34;,
          &#34;columns&#34;: &#34;*&#34;,
          &#34;from&#34;: [
              {&#34;table&#34;: &#34;patient&#34;}
          ],
        }, all_results=True)

        &#34;&#34;&#34;
        query = build_query(query, **query_kwargs)

        if log:
            print(json.dumps(query, indent=4))

        if FhirAggregation.is_aggregation_query(query):
            response = execute_single_fhir_dsl(query, auth_args=auth_args)
            return FhirAggregation.from_response(response)

        if all_results:
            return with_progress(
                lambda: tqdm(total=MAX_RESULT_SIZE),
                lambda progress: recursive_execute_fhir_dsl(
                    {
                        &#34;limit&#34;: [
                            {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: 0},
                            # Make window size smaller than maximum to reduce
                            # pressure on API
                            {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: DEFAULT_SCROLL_SIZE},
                        ],
                        **query,
                    },
                    scroll=all_results,
                    progress=progress,
                    callback=callback,
                    auth_args=auth_args,
                    max_pages=max_pages,
                ),
            )

        return recursive_execute_fhir_dsl(
            query,
            scroll=all_results,
            callback=callback,
            auth_args=auth_args,
            max_pages=max_pages,
        )

    @staticmethod
    def execute_paging_api(
        path: str,
        params: dict = {},
        http_verb: str = &#34;GET&#34;,
        transform: Callable[[pd.DataFrame], pd.DataFrame] = identity,
        all_results: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Optional[int] = None,
        page_size: Optional[int] = None,
        log: bool = False,
        raw: bool = False,
        ignore_cache: bool = False,
        show_progress: bool = True,
        progress: Optional[tqdm] = None,
        item_key: str = &#34;items&#34;,
        try_count: bool = True,
    ):
        &#34;&#34;&#34;Execute a API query that pages through results

        See https://docs.us.lifeomic.com/api/?shell#lifeomic-core-api-genomics
        for example

        Attributes
        ----------
        path : str
            The API path to hit
            (Special tokens: `:project_id`)

        params : dict
            The parameters to include with request

        http_verb : str
            The HTTP method to use

        all_results : bool = False
            Retrieve sample of results (25) or entire set of records

        auth_args : Auth, dict
            Additional arguments for authentication

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        page_size : int
            The number of records to fetch per page

        log : bool = False
            Whether to log some diagnostic statements for debugging

        progress : Optional[tqdm] = None
            Override the given progress indicator

        item_key : str
            The key to find the results underneath (usually &#34;items&#34; but not always)

        try_count : bool
            Whether to try and send a &#34;count&#34; param to update the progress bar

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_paging_api(
                &#34;genomics/projects/:project_id/tests&#34;,
                params={
                    &#34;patientId&#34;: &#34;&lt;patient-uuid&gt;&#34;
                }
            )

        &#34;&#34;&#34;

        auth = Auth(auth_args)

        params = clean_params(params)

        # Do not pull project_id if not in URL (which throws error if project not selected)
        if &#34;project_id&#34; in path:
            path = path.replace(&#34;:project_id&#34;, auth.project_id)

        query = {&#34;path&#34;: path, &#34;method&#34;: http_verb, &#34;params&#34;: params}

        if all_results and page_size is None:
            # Default to 100 if not provided but getting all results
            page_size = 100

        if log:
            print(json.dumps(query, indent=4))

        use_cache = (
            (not ignore_cache)
            and (not raw)
            and all_results
            and (max_pages is None)
        )

        if use_cache and APICache.does_cache_for_query_exist(query):
            return APICache.load_cache_for_query(query)

        callback = (
            APICache.build_cache_callback(query, transform, nested_key=None)
            if use_cache
            else None
        )

        results = with_progress(
            lambda: (progress if progress is not None else tqdm())
            if show_progress
            else None,
            lambda progress: recursive_paging_api_call(
                path,
                params=params,
                http_verb=http_verb,
                callback=callback,
                scroll=all_results or (max_pages is not None),
                max_pages=max_pages,
                page_size=page_size,
                log=log,
                auth_args=auth_args,
                progress=progress,
                item_key=item_key,
                try_count=try_count,
            ),
        )

        df = pd.DataFrame(results)

        if raw:
            return df

        return transform(df)

    @staticmethod
    def execute_fhir_dsl_with_options(
        query: dict,
        transform: Callable[[pd.DataFrame], pd.DataFrame],
        all_results: bool,
        raw: bool,
        query_overrides: dict,
        auth_args: Auth,
        ignore_cache: bool,
        max_pages: Union[int, None],
        log: bool = False,
        **query_kwargs,
    ):
        query = build_query({**query, **query_overrides}, **query_kwargs)

        if log:
            print(json.dumps(query, indent=4))

        use_cache = (
            (not ignore_cache)
            and (not raw)
            and (all_results or FhirAggregation.is_aggregation_query(query))
            and (max_pages is None)
        )

        if use_cache and APICache.does_cache_for_query_exist(
            query, namespace=FHIR_DSL
        ):
            return APICache.load_cache_for_query(query, namespace=FHIR_DSL)

        callback = (
            APICache.build_cache_callback(query, transform, namespace=FHIR_DSL)
            if use_cache
            else None
        )

        results = Query.execute_fhir_dsl(
            query,
            all_results,
            auth_args,
            callback=callback,
            max_pages=max_pages,
        )

        if isinstance(results, FhirAggregation):
            # Cache isn&#39;t written in batches so we need to explicitly do it here
            if use_cache:
                APICache.write_agg(query, results)

            return results

        if isinstance(results, pd.DataFrame):
            return results

        df = pd.DataFrame(map(lambda r: r[&#34;_source&#34;], results))

        if raw:
            return df

        return transform(df)

    @staticmethod
    def get_codes(
        table_name: str,
        code_fields: List[str],
        display_query: Optional[str] = None,
        sample_size: Optional[int] = None,
        **kwargs,
    ):
        &#34;&#34;&#34;Find FHIR codes with a display for a given table

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        code_fields : List[str]
            The fields of this table that contain a system, code, and display

        display_query : Optional[str]
            Part of the code&#39;s display to match (will try to extract full code
            if passed)

        sample_size : Optional[int]
            Override the search size for finding codes (may miss codes on later
            records)

        kwargs : dict
            Arguments to pass to `phc.easy.query.Query.execute_composite_aggregations`

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.get_codes(
            table_name=&#34;observation&#34;,
            code_fields=[&#34;meta.tag&#34;, &#34;code.coding&#34;],
            patient_id=&#34;&lt;my-patient-id&gt;&#34;
        )
        &#34;&#34;&#34;
        if len(code_fields) == 0:
            raise ValueError(&#34;No code columns specified.&#34;)

        def agg_composite_to_frame(prefix: str, data: dict):
            frame = pd.json_normalize(data[&#34;buckets&#34;])
            frame.columns = frame.columns.str.lstrip(&#34;key.&#34;)
            frame[&#34;field&#34;] = prefix
            return frame

        if display_query is not None:
            kwargs = {
                **kwargs,
                &#34;query_overrides&#34;: {
                    &#34;where&#34;: {
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;query&#34;: {
                            &#34;multi_match&#34;: {
                                &#34;query&#34;: display_query,
                                &#34;fields&#34;: [
                                    f&#34;{key}.display&#34; for key in code_fields
                                ],
                            }
                        },
                    }
                },
            }

        results = Query.execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=[
                (
                    field,
                    [
                        {
                            &#34;display&#34;: {
                                &#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.display.keyword&#34;}
                            }
                        }
                    ],
                )
                for field in code_fields
            ],
            **kwargs,
        )

        agg_result = (
            pd.concat(
                [
                    agg_composite_to_frame(key, value)
                    for key, value in results.items()
                ]
            )
            .pipe(
                lambda df: df
                if len(df) == 0 or display_query is None
                # Poor man&#39;s way to filter only matching codes (since Elasticsearch
                # returns records which will include other codes)
                else df[
                    df[&#34;display&#34;]
                    .str.lower()
                    .str.contains(display_query.lower())
                ]
            )
            .pipe(
                lambda df: pd.DataFrame()
                if len(df) == 0
                else df.sort_values(&#34;doc_count&#34;, ascending=False).reset_index(
                    drop=True
                )
            )
        )

        if display_query is None or len(agg_result) == 0:
            return agg_result

        min_count = sample_size or agg_result.doc_count.sum()
        filtered_code_fields = agg_result.field.unique()

        # Shortcut: If one result, we just need to get the other associated
        # attributes of the code
        if len(agg_result) == 1:
            min_count = 1

        code_results = Query.execute_fhir_dsl(
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;from&#34;: [{&#34;table&#34;: table_name}],
                &#34;columns&#34;: [
                    {
                        &#34;expr&#34;: {
                            &#34;type&#34;: &#34;column_ref&#34;,
                            &#34;column&#34;: key.split(&#34;.&#34;)[0],
                        }
                    }
                    for key in filtered_code_fields
                ],
                &#34;where&#34;: {
                    &#34;type&#34;: &#34;elasticsearch&#34;,
                    &#34;query&#34;: {
                        &#34;multi_match&#34;: {
                            &#34;query&#34;: display_query,
                            &#34;fields&#34;: [
                                f&#34;{key}.display&#34; for key in filtered_code_fields
                            ],
                        }
                    },
                },
            },
            page_size=int(min_count % 9000),
            max_pages=int(math.ceil(min_count / 9000)),
            log=kwargs.get(&#34;log&#34;, False),
        )

        codes = extract_codes(
            map(lambda d: d[&#34;_source&#34;], code_results),
            display_query,
            code_fields,
        )

        if len(codes) == 0:
            return codes

        if len(codes) == codes.display.nunique():
            # If display values are unique, then the counts from Elasticsearch
            # are correct. We can therefore join them.
            codes = (
                codes.join(
                    agg_result[[&#34;display&#34;, &#34;doc_count&#34;]].set_index(&#34;display&#34;),
                    on=&#34;display&#34;,
                    how=&#34;outer&#34;,
                )
                .sort_values(&#34;doc_count&#34;, ascending=False)
                .reset_index(drop=True)
            )

            if len(codes[codes.field.isnull()]) &gt; 0:
                print(
                    &#34;Records with missing system/code values were not retrieved.&#34;
                )

            return codes

        return codes

    @staticmethod
    def execute_composite_aggregations(
        table_name: str,
        key_sources_pairs: List[Tuple[str, List[dict]]],
        batch_size: int = 100,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Union[int, None] = None,
        **query_kwargs,
    ):
        &#34;&#34;&#34;Count records by multiple fields

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        key_sources_pairs : str
            Pairs of keys and sources to pull composite results from

            Example Input:
                [
                    (&#34;meta.tag&#34;, [{&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.system.keyword&#34;}}])
                ]

        batch_size : int
            The size of each page from elasticsearch to use

        query_overrides : dict
            Parts of the FSS query to override
            (Note that passing certain values can cause the method to error out)

            Example aggregation query executed (can use log=True to inspect):
                {
                    &#34;type&#34;: &#34;select&#34;,
                    &#34;columns&#34;: [{
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            &#34;results&#34;: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: [{
                                        &#34;meta.tag&#34;: {
                                            &#34;terms&#34;: {
                                                &#34;field&#34;: &#34;meta.tag.system.keyword&#34;
                                            }
                                        }
                                    }],
                                    &#34;size&#34;: 100,
                                }
                            }
                        },
                    }],
                    &#34;from&#34;: [{&#34;table&#34;: &#34;observation&#34;}],
                }


        auth_args : Auth, dict
            Additional arguments for authentication

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        max_pages : int
            The number of pages to retrieve (useful if working with tons of records)

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. See :func:`~phc.easy.query.fhir_dsl_query.build_query`.

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.execute_composite_aggregations(
            table_name=&#34;observation&#34;,
            key_sources_pairs=[
                (&#34;meta.tag&#34;, [
                    {&#34;code&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.code.keyword&#34;}}},
                ]),
                (&#34;code.coding&#34;, [
                    {&#34;display&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;code.coding.display.keyword&#34;}}}
                ]),
            ]
        )
        &#34;&#34;&#34;
        if len(key_sources_pairs) == 0:
            raise ValueError(&#34;No aggregate composite terms specified.&#34;)

        return with_progress(
            tqdm,
            lambda progress: Query._recursive_execute_composite_aggregations(
                table_name=table_name,
                key_sources_pairs=key_sources_pairs,
                batch_size=batch_size,
                progress=progress,
                log=log,
                auth_args=auth_args,
                query_overrides=query_overrides,
                max_pages=max_pages,
                **query_kwargs,
            ),
        )

    @staticmethod
    def get_count_by_field(
        table_name: str,
        field: str,
        batch_size: int = 1000,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        **query_kwargs,
    ):
        &#34;&#34;&#34;Count records by a given field

        Attributes
        ----------
        table_name : str
            The FHIR Search Service table to retrieve from

        field : str
            The field name to count the values of (e.g. &#34;subject.reference&#34;)

        batch_size : int
            The size of each page from elasticsearch to use

        query_overrides : dict
            Parts of the FSS query to override
            (Note that passing certain values can cause the method to error out)

            The aggregation query is similar to this:
                {
                    &#34;type&#34;: &#34;select&#34;,
                    &#34;columns&#34;: [{
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            &#34;results&#34;: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: [{
                                        &#34;value&#34;: {
                                            &#34;terms&#34;: {
                                                &#34;field&#34;: &#34;gender.keyword&#34;
                                            }
                                        }
                                    }],
                                    &#34;size&#34;: 100,
                                }
                            }
                        },
                    }],
                    &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
                }


        auth_args : Auth, dict
            Additional arguments for authentication

        log : bool = False
            Whether to log the elasticsearch query sent to the server

        query_kwargs : dict
            Arguments to pass to build_query such as patient_id, patient_ids,
            and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

        Examples
        --------
        &gt;&gt;&gt; import phc.easy as phc
        &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
        &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
        &gt;&gt;&gt; phc.Query.get_count_by_field(
            table_name=&#34;patient&#34;,
            field=&#34;gender&#34;
        )
        &#34;&#34;&#34;
        data = Query.execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=[
                (
                    &#34;results&#34;,
                    [{&#34;value&#34;: {&#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.keyword&#34;}}}],
                )
            ],
            batch_size=batch_size,
            log=log,
            auth_args=auth_args,
            query_overrides=query_overrides,
            **query_kwargs,
        )

        return pd.DataFrame(
            [
                {field: r[&#34;key&#34;][&#34;value&#34;], &#34;doc_count&#34;: r[&#34;doc_count&#34;]}
                for r in data[&#34;results&#34;][&#34;buckets&#34;]
            ]
        )

    @staticmethod
    def execute_ga4gh(
        query: dict, all_results: bool = False, auth_args: dict = Auth.shared()
    ) -&gt; pd.DataFrame:
        auth = Auth(auth_args)
        client = BaseClient(auth.session())
        path = query[&#34;path&#34;]
        http_verb = query.get(&#34;http_verb&#34;, &#34;POST&#34;)
        results_key = query[&#34;results_key&#34;]
        params = {
            **{&#34;datasetIds&#34;: [auth.project_id]},
            **{
                k: v for k, v in query.items() if k not in [&#34;path&#34;, &#34;http_verb&#34;]
            },
        }

        return recursive_execute_ga4gh(
            auth=auth,
            client=client,
            path=path,
            http_verb=http_verb,
            results_key=results_key,
            params=params,
            scroll=all_results,
        )

    @staticmethod
    def _recursive_execute_composite_aggregations(
        table_name: str,
        key_sources_pairs: List[Tuple[str, List[dict]]],
        batch_size: int = 100,
        progress: Union[tqdm, None] = None,
        query_overrides: dict = {},
        log: bool = False,
        auth_args: Auth = Auth.shared(),
        max_pages: Union[int, None] = None,
        _current_page: int = 1,
        _prev_results: dict = {},
        _after_keys: dict = {},
        **query_kwargs,
    ):
        aggregation = Query.execute_fhir_dsl(
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;columns&#34;: [
                    {
                        &#34;type&#34;: &#34;elasticsearch&#34;,
                        &#34;aggregations&#34;: {
                            key: {
                                &#34;composite&#34;: {
                                    &#34;sources&#34;: sources,
                                    &#34;size&#34;: batch_size,
                                    **(
                                        {&#34;after&#34;: _after_keys[key]}
                                        if key in _after_keys
                                        else {}
                                    ),
                                }
                            }
                            for key, sources in key_sources_pairs
                            if (len(_after_keys) == 0) or (key in _after_keys)
                        },
                    }
                ],
                &#34;from&#34;: [{&#34;table&#34;: table_name}],
                **query_overrides,
            },
            auth_args=auth_args,
            log=log,
            **query_kwargs,
        )

        current_results = aggregation.data
        results = FhirAggregation.reduce_composite_results(
            _prev_results, current_results
        )

        if (progress is not None) and (_current_page == 1) and max_pages:
            progress.reset(max_pages)

        if progress is not None:
            # Update by count or pages (if max_pages specified)
            progress.update(
                1
                if max_pages
                else FhirAggregation.count_composite_results(current_results)
            )

        after_keys = FhirAggregation.find_composite_after_keys(
            current_results, batch_size
        )

        if len(after_keys) == 0 or (
            (max_pages is not None) and (_current_page &gt;= max_pages)
        ):
            print(
                f&#34;Retrieved {FhirAggregation.count_composite_results(results)} results&#34;
            )
            return results

        return Query._recursive_execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=key_sources_pairs,
            batch_size=batch_size,
            progress=progress,
            query_overrides=query_overrides,
            log=log,
            auth_args=auth_args,
            max_pages=max_pages,
            _current_page=_current_page + 1,
            _prev_results=results,
            _after_keys=after_keys,
            **query_kwargs,
        )</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="phc.easy.query.Query.execute_composite_aggregations"><code class="name flex">
<span>def <span class="ident">execute_composite_aggregations</span></span>(<span>table_name:str, key_sources_pairs:List[Tuple[str,List[dict]]], batch_size:int=100, query_overrides:dict={}, log:bool=False, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>=&lt;phc.easy.auth.Auth object&gt;, max_pages:Union[int,NoneType]=None, **query_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Count records by multiple fields</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>table_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The FHIR Search Service table to retrieve from</dd>
<dt><strong><code>key_sources_pairs</code></strong> :&ensp;<code>str</code></dt>
<dd>
<p>Pairs of keys and sources to pull composite results from</p>
<p>Example Input:
[
("meta.tag", [{"terms": {"field": "meta.tag.system.keyword"}}])
]</p>
</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The size of each page from elasticsearch to use</dd>
<dt><strong><code>query_overrides</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>Parts of the FSS query to override
(Note that passing certain values can cause the method to error out)</p>
<p>Example aggregation query executed (can use log=True to inspect):
{
"type": "select",
"columns": [{
"type": "elasticsearch",
"aggregations": {
"results": {
"composite": {
"sources": [{
"meta.tag": {
"terms": {
"field": "meta.tag.system.keyword"
}
}
}],
"size": 100,
}
}
},
}],
"from": [{"table": "observation"}],
}</p>
</dd>
<dt><strong><code>auth_args</code></strong> :&ensp;<code>Auth, dict</code></dt>
<dd>Additional arguments for authentication</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool = False</code></dt>
<dd>Whether to log the elasticsearch query sent to the server</dd>
<dt><strong><code>max_pages</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pages to retrieve (useful if working with tons of records)</dd>
<dt><strong><code>query_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Arguments to pass to build_query such as patient_id, patient_ids,
and patient_key. See :func:<code>~phc.easy.query.fhir_dsl_query.build_query</code>.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.execute_composite_aggregations(
    table_name=&quot;observation&quot;,
    key_sources_pairs=[
        (&quot;meta.tag&quot;, [
            {&quot;code&quot;: {&quot;terms&quot;: {&quot;field&quot;: &quot;meta.tag.code.keyword&quot;}}},
        ]),
        (&quot;code.coding&quot;, [
            {&quot;display&quot;: {&quot;terms&quot;: {&quot;field&quot;: &quot;code.coding.display.keyword&quot;}}}
        ]),
    ]
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def execute_composite_aggregations(
    table_name: str,
    key_sources_pairs: List[Tuple[str, List[dict]]],
    batch_size: int = 100,
    query_overrides: dict = {},
    log: bool = False,
    auth_args: Auth = Auth.shared(),
    max_pages: Union[int, None] = None,
    **query_kwargs,
):
    &#34;&#34;&#34;Count records by multiple fields

    Attributes
    ----------
    table_name : str
        The FHIR Search Service table to retrieve from

    key_sources_pairs : str
        Pairs of keys and sources to pull composite results from

        Example Input:
            [
                (&#34;meta.tag&#34;, [{&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.system.keyword&#34;}}])
            ]

    batch_size : int
        The size of each page from elasticsearch to use

    query_overrides : dict
        Parts of the FSS query to override
        (Note that passing certain values can cause the method to error out)

        Example aggregation query executed (can use log=True to inspect):
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;columns&#34;: [{
                    &#34;type&#34;: &#34;elasticsearch&#34;,
                    &#34;aggregations&#34;: {
                        &#34;results&#34;: {
                            &#34;composite&#34;: {
                                &#34;sources&#34;: [{
                                    &#34;meta.tag&#34;: {
                                        &#34;terms&#34;: {
                                            &#34;field&#34;: &#34;meta.tag.system.keyword&#34;
                                        }
                                    }
                                }],
                                &#34;size&#34;: 100,
                            }
                        }
                    },
                }],
                &#34;from&#34;: [{&#34;table&#34;: &#34;observation&#34;}],
            }


    auth_args : Auth, dict
        Additional arguments for authentication

    log : bool = False
        Whether to log the elasticsearch query sent to the server

    max_pages : int
        The number of pages to retrieve (useful if working with tons of records)

    query_kwargs : dict
        Arguments to pass to build_query such as patient_id, patient_ids,
        and patient_key. See :func:`~phc.easy.query.fhir_dsl_query.build_query`.

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.execute_composite_aggregations(
        table_name=&#34;observation&#34;,
        key_sources_pairs=[
            (&#34;meta.tag&#34;, [
                {&#34;code&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;meta.tag.code.keyword&#34;}}},
            ]),
            (&#34;code.coding&#34;, [
                {&#34;display&#34;: {&#34;terms&#34;: {&#34;field&#34;: &#34;code.coding.display.keyword&#34;}}}
            ]),
        ]
    )
    &#34;&#34;&#34;
    if len(key_sources_pairs) == 0:
        raise ValueError(&#34;No aggregate composite terms specified.&#34;)

    return with_progress(
        tqdm,
        lambda progress: Query._recursive_execute_composite_aggregations(
            table_name=table_name,
            key_sources_pairs=key_sources_pairs,
            batch_size=batch_size,
            progress=progress,
            log=log,
            auth_args=auth_args,
            query_overrides=query_overrides,
            max_pages=max_pages,
            **query_kwargs,
        ),
    )</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.execute_fhir_dsl"><code class="name flex">
<span>def <span class="ident">execute_fhir_dsl</span></span>(<span>query:dict, all_results:bool=False, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>=&lt;phc.easy.auth.Auth object&gt;, callback:Union[Callable[[Any,bool],NoneType],NoneType]=None, max_pages:Union[int,NoneType]=None, log:bool=False, **query_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute a FHIR query with the DSL</p>
<p>See <a href="https://docs.us.lifeomic.com/development/fhir-service/dsl/">https://docs.us.lifeomic.com/development/fhir-service/dsl/</a></p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>query</code></strong> :&ensp;<code>dict</code></dt>
<dd>The FHIR query to run (is a superset of elasticsearch)</dd>
<dt><strong><code>all_results</code></strong> :&ensp;<code>bool</code></dt>
<dd>Return all results by scrolling through mutliple pages of data
(Limit is ignored if provided)</dd>
<dt><strong><code>auth_args</code></strong> :&ensp;<code>Auth, dict</code></dt>
<dd>Additional arguments for authentication</dd>
<dt><strong><code>callback</code></strong> :&ensp;<code>Callable[[Any, bool], None] (optional)</code></dt>
<dd>
<p>A progress function that is invoked for each batch. When the second
argument passed is true, then the result of the callback function is
used as the return value. This is useful if writing results out to a
file and then returning the completed result from that file.</p>
<p>Example:</p>
<pre><code>def handle_batch(batch, is_finished):
    print(len(batch))
    if is_finished:
        return "batch finished
</code></pre>
</dd>
<dt><strong><code>max_pages</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pages to retrieve (useful if working with tons of records)</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool = False</code></dt>
<dd>Whether to log the elasticsearch query sent to the server</dd>
<dt><strong><code>query_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Arguments to pass to build_query such as patient_id, patient_ids,
and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.execute_fhir_dsl({
  &quot;type&quot;: &quot;select&quot;,
  &quot;columns&quot;: &quot;*&quot;,
  &quot;from&quot;: [
      {&quot;table&quot;: &quot;patient&quot;}
  ],
}, all_results=True)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def execute_fhir_dsl(
    query: dict,
    all_results: bool = False,
    auth_args: Auth = Auth.shared(),
    callback: Union[Callable[[Any, bool], None], None] = None,
    max_pages: Union[int, None] = None,
    log: bool = False,
    **query_kwargs,
):
    &#34;&#34;&#34;Execute a FHIR query with the DSL

    See https://docs.us.lifeomic.com/development/fhir-service/dsl/

    Attributes
    ----------
    query : dict
        The FHIR query to run (is a superset of elasticsearch)

    all_results : bool
        Return all results by scrolling through mutliple pages of data
        (Limit is ignored if provided)

    auth_args : Auth, dict
        Additional arguments for authentication

    callback : Callable[[Any, bool], None] (optional)
        A progress function that is invoked for each batch. When the second
        argument passed is true, then the result of the callback function is
        used as the return value. This is useful if writing results out to a
        file and then returning the completed result from that file.

        Example:

            def handle_batch(batch, is_finished):
                print(len(batch))
                if is_finished:
                    return &#34;batch finished

    max_pages : int
        The number of pages to retrieve (useful if working with tons of records)

    log : bool = False
        Whether to log the elasticsearch query sent to the server

    query_kwargs : dict
        Arguments to pass to build_query such as patient_id, patient_ids,
        and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.execute_fhir_dsl({
      &#34;type&#34;: &#34;select&#34;,
      &#34;columns&#34;: &#34;*&#34;,
      &#34;from&#34;: [
          {&#34;table&#34;: &#34;patient&#34;}
      ],
    }, all_results=True)

    &#34;&#34;&#34;
    query = build_query(query, **query_kwargs)

    if log:
        print(json.dumps(query, indent=4))

    if FhirAggregation.is_aggregation_query(query):
        response = execute_single_fhir_dsl(query, auth_args=auth_args)
        return FhirAggregation.from_response(response)

    if all_results:
        return with_progress(
            lambda: tqdm(total=MAX_RESULT_SIZE),
            lambda progress: recursive_execute_fhir_dsl(
                {
                    &#34;limit&#34;: [
                        {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: 0},
                        # Make window size smaller than maximum to reduce
                        # pressure on API
                        {&#34;type&#34;: &#34;number&#34;, &#34;value&#34;: DEFAULT_SCROLL_SIZE},
                    ],
                    **query,
                },
                scroll=all_results,
                progress=progress,
                callback=callback,
                auth_args=auth_args,
                max_pages=max_pages,
            ),
        )

    return recursive_execute_fhir_dsl(
        query,
        scroll=all_results,
        callback=callback,
        auth_args=auth_args,
        max_pages=max_pages,
    )</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.execute_fhir_dsl_with_options"><code class="name flex">
<span>def <span class="ident">execute_fhir_dsl_with_options</span></span>(<span>query:dict, transform:Callable[[pandas.core.frame.DataFrame],pandas.core.frame.DataFrame], all_results:bool, raw:bool, query_overrides:dict, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>, ignore_cache:bool, max_pages:Union[int,NoneType], log:bool=False, **query_kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def execute_fhir_dsl_with_options(
    query: dict,
    transform: Callable[[pd.DataFrame], pd.DataFrame],
    all_results: bool,
    raw: bool,
    query_overrides: dict,
    auth_args: Auth,
    ignore_cache: bool,
    max_pages: Union[int, None],
    log: bool = False,
    **query_kwargs,
):
    query = build_query({**query, **query_overrides}, **query_kwargs)

    if log:
        print(json.dumps(query, indent=4))

    use_cache = (
        (not ignore_cache)
        and (not raw)
        and (all_results or FhirAggregation.is_aggregation_query(query))
        and (max_pages is None)
    )

    if use_cache and APICache.does_cache_for_query_exist(
        query, namespace=FHIR_DSL
    ):
        return APICache.load_cache_for_query(query, namespace=FHIR_DSL)

    callback = (
        APICache.build_cache_callback(query, transform, namespace=FHIR_DSL)
        if use_cache
        else None
    )

    results = Query.execute_fhir_dsl(
        query,
        all_results,
        auth_args,
        callback=callback,
        max_pages=max_pages,
    )

    if isinstance(results, FhirAggregation):
        # Cache isn&#39;t written in batches so we need to explicitly do it here
        if use_cache:
            APICache.write_agg(query, results)

        return results

    if isinstance(results, pd.DataFrame):
        return results

    df = pd.DataFrame(map(lambda r: r[&#34;_source&#34;], results))

    if raw:
        return df

    return transform(df)</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.execute_ga4gh"><code class="name flex">
<span>def <span class="ident">execute_ga4gh</span></span>(<span>query:dict, all_results:bool=False, auth_args:dict=&lt;phc.easy.auth.Auth object&gt;) >pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def execute_ga4gh(
    query: dict, all_results: bool = False, auth_args: dict = Auth.shared()
) -&gt; pd.DataFrame:
    auth = Auth(auth_args)
    client = BaseClient(auth.session())
    path = query[&#34;path&#34;]
    http_verb = query.get(&#34;http_verb&#34;, &#34;POST&#34;)
    results_key = query[&#34;results_key&#34;]
    params = {
        **{&#34;datasetIds&#34;: [auth.project_id]},
        **{
            k: v for k, v in query.items() if k not in [&#34;path&#34;, &#34;http_verb&#34;]
        },
    }

    return recursive_execute_ga4gh(
        auth=auth,
        client=client,
        path=path,
        http_verb=http_verb,
        results_key=results_key,
        params=params,
        scroll=all_results,
    )</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.execute_paging_api"><code class="name flex">
<span>def <span class="ident">execute_paging_api</span></span>(<span>path:str, params:dict={}, http_verb:str='GET', transform:Callable[[pandas.core.frame.DataFrame],pandas.core.frame.DataFrame]=&lt;function identity&gt;, all_results:bool=False, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>=&lt;phc.easy.auth.Auth object&gt;, max_pages:Union[int,NoneType]=None, page_size:Union[int,NoneType]=None, log:bool=False, raw:bool=False, ignore_cache:bool=False, show_progress:bool=True, progress:Union[tqdm.std.tqdm,NoneType]=None, item_key:str='items', try_count:bool=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute a API query that pages through results</p>
<p>See <a href="https://docs.us.lifeomic.com/api/?shell#lifeomic-core-api-genomics">https://docs.us.lifeomic.com/api/?shell#lifeomic-core-api-genomics</a>
for example</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code></dt>
<dd>The API path to hit
(Special tokens: <code>:project_id</code>)</dd>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>The parameters to include with request</dd>
<dt><strong><code>http_verb</code></strong> :&ensp;<code>str</code></dt>
<dd>The HTTP method to use</dd>
<dt><strong><code>all_results</code></strong> :&ensp;<code>bool = False</code></dt>
<dd>Retrieve sample of results (25) or entire set of records</dd>
<dt><strong><code>auth_args</code></strong> :&ensp;<code>Auth, dict</code></dt>
<dd>Additional arguments for authentication</dd>
<dt><strong><code>max_pages</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of pages to retrieve (useful if working with tons of records)</dd>
<dt><strong><code>page_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of records to fetch per page</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool = False</code></dt>
<dd>Whether to log some diagnostic statements for debugging</dd>
<dt><strong><code>progress</code></strong> :&ensp;<code>Optional[tqdm] = None</code></dt>
<dd>Override the given progress indicator</dd>
<dt><strong><code>item_key</code></strong> :&ensp;<code>str</code></dt>
<dd>The key to find the results underneath (usually "items" but not always)</dd>
<dt><strong><code>try_count</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to try and send a "count" param to update the progress bar</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.execute_paging_api(
        &quot;genomics/projects/:project_id/tests&quot;,
        params={
            &quot;patientId&quot;: &quot;&lt;patient-uuid&gt;&quot;
        }
    )
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def execute_paging_api(
    path: str,
    params: dict = {},
    http_verb: str = &#34;GET&#34;,
    transform: Callable[[pd.DataFrame], pd.DataFrame] = identity,
    all_results: bool = False,
    auth_args: Auth = Auth.shared(),
    max_pages: Optional[int] = None,
    page_size: Optional[int] = None,
    log: bool = False,
    raw: bool = False,
    ignore_cache: bool = False,
    show_progress: bool = True,
    progress: Optional[tqdm] = None,
    item_key: str = &#34;items&#34;,
    try_count: bool = True,
):
    &#34;&#34;&#34;Execute a API query that pages through results

    See https://docs.us.lifeomic.com/api/?shell#lifeomic-core-api-genomics
    for example

    Attributes
    ----------
    path : str
        The API path to hit
        (Special tokens: `:project_id`)

    params : dict
        The parameters to include with request

    http_verb : str
        The HTTP method to use

    all_results : bool = False
        Retrieve sample of results (25) or entire set of records

    auth_args : Auth, dict
        Additional arguments for authentication

    max_pages : int
        The number of pages to retrieve (useful if working with tons of records)

    page_size : int
        The number of records to fetch per page

    log : bool = False
        Whether to log some diagnostic statements for debugging

    progress : Optional[tqdm] = None
        Override the given progress indicator

    item_key : str
        The key to find the results underneath (usually &#34;items&#34; but not always)

    try_count : bool
        Whether to try and send a &#34;count&#34; param to update the progress bar

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.execute_paging_api(
            &#34;genomics/projects/:project_id/tests&#34;,
            params={
                &#34;patientId&#34;: &#34;&lt;patient-uuid&gt;&#34;
            }
        )

    &#34;&#34;&#34;

    auth = Auth(auth_args)

    params = clean_params(params)

    # Do not pull project_id if not in URL (which throws error if project not selected)
    if &#34;project_id&#34; in path:
        path = path.replace(&#34;:project_id&#34;, auth.project_id)

    query = {&#34;path&#34;: path, &#34;method&#34;: http_verb, &#34;params&#34;: params}

    if all_results and page_size is None:
        # Default to 100 if not provided but getting all results
        page_size = 100

    if log:
        print(json.dumps(query, indent=4))

    use_cache = (
        (not ignore_cache)
        and (not raw)
        and all_results
        and (max_pages is None)
    )

    if use_cache and APICache.does_cache_for_query_exist(query):
        return APICache.load_cache_for_query(query)

    callback = (
        APICache.build_cache_callback(query, transform, nested_key=None)
        if use_cache
        else None
    )

    results = with_progress(
        lambda: (progress if progress is not None else tqdm())
        if show_progress
        else None,
        lambda progress: recursive_paging_api_call(
            path,
            params=params,
            http_verb=http_verb,
            callback=callback,
            scroll=all_results or (max_pages is not None),
            max_pages=max_pages,
            page_size=page_size,
            log=log,
            auth_args=auth_args,
            progress=progress,
            item_key=item_key,
            try_count=try_count,
        ),
    )

    df = pd.DataFrame(results)

    if raw:
        return df

    return transform(df)</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.find_count_of_dsl_query"><code class="name flex">
<span>def <span class="ident">find_count_of_dsl_query</span></span>(<span>query:dict, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>=&lt;phc.easy.auth.Auth object&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Find count of a given dsl query</p>
<p>See <a href="https://docs.us.lifeomic.com/development/fhir-service/dsl/">https://docs.us.lifeomic.com/development/fhir-service/dsl/</a></p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>query</code></strong> :&ensp;<code>dict</code></dt>
<dd>The FHIR query to run a count against</dd>
<dt><strong><code>auth_args</code></strong> :&ensp;<code>Auth, dict</code></dt>
<dd>Additional arguments for authentication</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.find_count_of_dsl_query({
  &quot;type&quot;: &quot;select&quot;,
  &quot;columns&quot;: &quot;*&quot;,
  &quot;from&quot;: [{&quot;table&quot;: &quot;patient&quot;}],
})
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def find_count_of_dsl_query(query: dict, auth_args: Auth = Auth.shared()):
    &#34;&#34;&#34;Find count of a given dsl query

    See https://docs.us.lifeomic.com/development/fhir-service/dsl/

    Attributes
    ----------
    query : dict
        The FHIR query to run a count against

    auth_args : Auth, dict
        Additional arguments for authentication

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.find_count_of_dsl_query({
      &#34;type&#34;: &#34;select&#34;,
      &#34;columns&#34;: &#34;*&#34;,
      &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
    })
    &#34;&#34;&#34;
    if FhirAggregation.is_aggregation_query(query):
        raise ValueError(&#34;Count is not support for aggregation queries.&#34;)

    auth = Auth(auth_args)
    fhir = Fhir(auth.session())

    response = fhir.execute_es(
        auth.project_id, build_query(query, page_size=1), scroll=&#34;true&#34;
    )

    return response.data[&#34;hits&#34;][&#34;total&#34;][&#34;value&#34;]</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.get_codes"><code class="name flex">
<span>def <span class="ident">get_codes</span></span>(<span>table_name:str, code_fields:List[str], display_query:Union[str,NoneType]=None, sample_size:Union[int,NoneType]=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Find FHIR codes with a display for a given table</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>table_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The FHIR Search Service table to retrieve from</dd>
<dt><strong><code>code_fields</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>The fields of this table that contain a system, code, and display</dd>
<dt><strong><code>display_query</code></strong> :&ensp;<code>Optional[str]</code></dt>
<dd>Part of the code's display to match (will try to extract full code
if passed)</dd>
<dt><strong><code>sample_size</code></strong> :&ensp;<code>Optional[int]</code></dt>
<dd>Override the search size for finding codes (may miss codes on later
records)</dd>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Arguments to pass to <code><a title="phc.easy.query.Query.execute_composite_aggregations" href="#phc.easy.query.Query.execute_composite_aggregations">Query.execute_composite_aggregations()</a></code></dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.get_codes(
    table_name=&quot;observation&quot;,
    code_fields=[&quot;meta.tag&quot;, &quot;code.coding&quot;],
    patient_id=&quot;&lt;my-patient-id&gt;&quot;
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_codes(
    table_name: str,
    code_fields: List[str],
    display_query: Optional[str] = None,
    sample_size: Optional[int] = None,
    **kwargs,
):
    &#34;&#34;&#34;Find FHIR codes with a display for a given table

    Attributes
    ----------
    table_name : str
        The FHIR Search Service table to retrieve from

    code_fields : List[str]
        The fields of this table that contain a system, code, and display

    display_query : Optional[str]
        Part of the code&#39;s display to match (will try to extract full code
        if passed)

    sample_size : Optional[int]
        Override the search size for finding codes (may miss codes on later
        records)

    kwargs : dict
        Arguments to pass to `phc.easy.query.Query.execute_composite_aggregations`

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.get_codes(
        table_name=&#34;observation&#34;,
        code_fields=[&#34;meta.tag&#34;, &#34;code.coding&#34;],
        patient_id=&#34;&lt;my-patient-id&gt;&#34;
    )
    &#34;&#34;&#34;
    if len(code_fields) == 0:
        raise ValueError(&#34;No code columns specified.&#34;)

    def agg_composite_to_frame(prefix: str, data: dict):
        frame = pd.json_normalize(data[&#34;buckets&#34;])
        frame.columns = frame.columns.str.lstrip(&#34;key.&#34;)
        frame[&#34;field&#34;] = prefix
        return frame

    if display_query is not None:
        kwargs = {
            **kwargs,
            &#34;query_overrides&#34;: {
                &#34;where&#34;: {
                    &#34;type&#34;: &#34;elasticsearch&#34;,
                    &#34;query&#34;: {
                        &#34;multi_match&#34;: {
                            &#34;query&#34;: display_query,
                            &#34;fields&#34;: [
                                f&#34;{key}.display&#34; for key in code_fields
                            ],
                        }
                    },
                }
            },
        }

    results = Query.execute_composite_aggregations(
        table_name=table_name,
        key_sources_pairs=[
            (
                field,
                [
                    {
                        &#34;display&#34;: {
                            &#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.display.keyword&#34;}
                        }
                    }
                ],
            )
            for field in code_fields
        ],
        **kwargs,
    )

    agg_result = (
        pd.concat(
            [
                agg_composite_to_frame(key, value)
                for key, value in results.items()
            ]
        )
        .pipe(
            lambda df: df
            if len(df) == 0 or display_query is None
            # Poor man&#39;s way to filter only matching codes (since Elasticsearch
            # returns records which will include other codes)
            else df[
                df[&#34;display&#34;]
                .str.lower()
                .str.contains(display_query.lower())
            ]
        )
        .pipe(
            lambda df: pd.DataFrame()
            if len(df) == 0
            else df.sort_values(&#34;doc_count&#34;, ascending=False).reset_index(
                drop=True
            )
        )
    )

    if display_query is None or len(agg_result) == 0:
        return agg_result

    min_count = sample_size or agg_result.doc_count.sum()
    filtered_code_fields = agg_result.field.unique()

    # Shortcut: If one result, we just need to get the other associated
    # attributes of the code
    if len(agg_result) == 1:
        min_count = 1

    code_results = Query.execute_fhir_dsl(
        {
            &#34;type&#34;: &#34;select&#34;,
            &#34;from&#34;: [{&#34;table&#34;: table_name}],
            &#34;columns&#34;: [
                {
                    &#34;expr&#34;: {
                        &#34;type&#34;: &#34;column_ref&#34;,
                        &#34;column&#34;: key.split(&#34;.&#34;)[0],
                    }
                }
                for key in filtered_code_fields
            ],
            &#34;where&#34;: {
                &#34;type&#34;: &#34;elasticsearch&#34;,
                &#34;query&#34;: {
                    &#34;multi_match&#34;: {
                        &#34;query&#34;: display_query,
                        &#34;fields&#34;: [
                            f&#34;{key}.display&#34; for key in filtered_code_fields
                        ],
                    }
                },
            },
        },
        page_size=int(min_count % 9000),
        max_pages=int(math.ceil(min_count / 9000)),
        log=kwargs.get(&#34;log&#34;, False),
    )

    codes = extract_codes(
        map(lambda d: d[&#34;_source&#34;], code_results),
        display_query,
        code_fields,
    )

    if len(codes) == 0:
        return codes

    if len(codes) == codes.display.nunique():
        # If display values are unique, then the counts from Elasticsearch
        # are correct. We can therefore join them.
        codes = (
            codes.join(
                agg_result[[&#34;display&#34;, &#34;doc_count&#34;]].set_index(&#34;display&#34;),
                on=&#34;display&#34;,
                how=&#34;outer&#34;,
            )
            .sort_values(&#34;doc_count&#34;, ascending=False)
            .reset_index(drop=True)
        )

        if len(codes[codes.field.isnull()]) &gt; 0:
            print(
                &#34;Records with missing system/code values were not retrieved.&#34;
            )

        return codes

    return codes</code></pre>
</details>
</dd>
<dt id="phc.easy.query.Query.get_count_by_field"><code class="name flex">
<span>def <span class="ident">get_count_by_field</span></span>(<span>table_name:str, field:str, batch_size:int=1000, query_overrides:dict={}, log:bool=False, auth_args:<a title="phc.easy.auth.Auth" href="../auth.html#phc.easy.auth.Auth">Auth</a>=&lt;phc.easy.auth.Auth object&gt;, **query_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Count records by a given field</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>table_name</code></strong> :&ensp;<code>str</code></dt>
<dd>The FHIR Search Service table to retrieve from</dd>
<dt><strong><code>field</code></strong> :&ensp;<code>str</code></dt>
<dd>The field name to count the values of (e.g. "subject.reference")</dd>
<dt><strong><code>batch_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The size of each page from elasticsearch to use</dd>
<dt><strong><code>query_overrides</code></strong> :&ensp;<code>dict</code></dt>
<dd>
<p>Parts of the FSS query to override
(Note that passing certain values can cause the method to error out)</p>
<p>The aggregation query is similar to this:
{
"type": "select",
"columns": [{
"type": "elasticsearch",
"aggregations": {
"results": {
"composite": {
"sources": [{
"value": {
"terms": {
"field": "gender.keyword"
}
}
}],
"size": 100,
}
}
},
}],
"from": [{"table": "patient"}],
}</p>
</dd>
<dt><strong><code>auth_args</code></strong> :&ensp;<code>Auth, dict</code></dt>
<dd>Additional arguments for authentication</dd>
<dt><strong><code>log</code></strong> :&ensp;<code>bool = False</code></dt>
<dd>Whether to log the elasticsearch query sent to the server</dd>
<dt><strong><code>query_kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Arguments to pass to build_query such as patient_id, patient_ids,
and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import phc.easy as phc
&gt;&gt;&gt; phc.Auth.set({ 'account': '&lt;your-account-name&gt;' })
&gt;&gt;&gt; phc.Project.set_current('My Project Name')
&gt;&gt;&gt; phc.Query.get_count_by_field(
    table_name=&quot;patient&quot;,
    field=&quot;gender&quot;
)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_count_by_field(
    table_name: str,
    field: str,
    batch_size: int = 1000,
    query_overrides: dict = {},
    log: bool = False,
    auth_args: Auth = Auth.shared(),
    **query_kwargs,
):
    &#34;&#34;&#34;Count records by a given field

    Attributes
    ----------
    table_name : str
        The FHIR Search Service table to retrieve from

    field : str
        The field name to count the values of (e.g. &#34;subject.reference&#34;)

    batch_size : int
        The size of each page from elasticsearch to use

    query_overrides : dict
        Parts of the FSS query to override
        (Note that passing certain values can cause the method to error out)

        The aggregation query is similar to this:
            {
                &#34;type&#34;: &#34;select&#34;,
                &#34;columns&#34;: [{
                    &#34;type&#34;: &#34;elasticsearch&#34;,
                    &#34;aggregations&#34;: {
                        &#34;results&#34;: {
                            &#34;composite&#34;: {
                                &#34;sources&#34;: [{
                                    &#34;value&#34;: {
                                        &#34;terms&#34;: {
                                            &#34;field&#34;: &#34;gender.keyword&#34;
                                        }
                                    }
                                }],
                                &#34;size&#34;: 100,
                            }
                        }
                    },
                }],
                &#34;from&#34;: [{&#34;table&#34;: &#34;patient&#34;}],
            }


    auth_args : Auth, dict
        Additional arguments for authentication

    log : bool = False
        Whether to log the elasticsearch query sent to the server

    query_kwargs : dict
        Arguments to pass to build_query such as patient_id, patient_ids,
        and patient_key. (See phc.easy.query.fhir_dsl_query.build_query)

    Examples
    --------
    &gt;&gt;&gt; import phc.easy as phc
    &gt;&gt;&gt; phc.Auth.set({ &#39;account&#39;: &#39;&lt;your-account-name&gt;&#39; })
    &gt;&gt;&gt; phc.Project.set_current(&#39;My Project Name&#39;)
    &gt;&gt;&gt; phc.Query.get_count_by_field(
        table_name=&#34;patient&#34;,
        field=&#34;gender&#34;
    )
    &#34;&#34;&#34;
    data = Query.execute_composite_aggregations(
        table_name=table_name,
        key_sources_pairs=[
            (
                &#34;results&#34;,
                [{&#34;value&#34;: {&#34;terms&#34;: {&#34;field&#34;: f&#34;{field}.keyword&#34;}}}],
            )
        ],
        batch_size=batch_size,
        log=log,
        auth_args=auth_args,
        query_overrides=query_overrides,
        **query_kwargs,
    )

    return pd.DataFrame(
        [
            {field: r[&#34;key&#34;][&#34;value&#34;], &#34;doc_count&#34;: r[&#34;doc_count&#34;]}
            for r in data[&#34;results&#34;][&#34;buckets&#34;]
        ]
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="PHC Home" href="https://lifeomic.github.io/phc-sdk-py/">
<img src="./phc.png" alt="">
</a>
</header>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="phc.easy" href="../index.html">phc.easy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="phc.easy.query.api_paging" href="api_paging.html">phc.easy.query.api_paging</a></code></li>
<li><code><a title="phc.easy.query.fhir_aggregation" href="fhir_aggregation.html">phc.easy.query.fhir_aggregation</a></code></li>
<li><code><a title="phc.easy.query.fhir_dsl" href="fhir_dsl.html">phc.easy.query.fhir_dsl</a></code></li>
<li><code><a title="phc.easy.query.fhir_dsl_query" href="fhir_dsl_query.html">phc.easy.query.fhir_dsl_query</a></code></li>
<li><code><a title="phc.easy.query.ga4gh" href="ga4gh.html">phc.easy.query.ga4gh</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="phc.easy.query.Query" href="#phc.easy.query.Query">Query</a></code></h4>
<ul class="">
<li><code><a title="phc.easy.query.Query.execute_composite_aggregations" href="#phc.easy.query.Query.execute_composite_aggregations">execute_composite_aggregations</a></code></li>
<li><code><a title="phc.easy.query.Query.execute_fhir_dsl" href="#phc.easy.query.Query.execute_fhir_dsl">execute_fhir_dsl</a></code></li>
<li><code><a title="phc.easy.query.Query.execute_fhir_dsl_with_options" href="#phc.easy.query.Query.execute_fhir_dsl_with_options">execute_fhir_dsl_with_options</a></code></li>
<li><code><a title="phc.easy.query.Query.execute_ga4gh" href="#phc.easy.query.Query.execute_ga4gh">execute_ga4gh</a></code></li>
<li><code><a title="phc.easy.query.Query.execute_paging_api" href="#phc.easy.query.Query.execute_paging_api">execute_paging_api</a></code></li>
<li><code><a title="phc.easy.query.Query.find_count_of_dsl_query" href="#phc.easy.query.Query.find_count_of_dsl_query">find_count_of_dsl_query</a></code></li>
<li><code><a title="phc.easy.query.Query.get_codes" href="#phc.easy.query.Query.get_codes">get_codes</a></code></li>
<li><code><a title="phc.easy.query.Query.get_count_by_field" href="#phc.easy.query.Query.get_count_by_field">get_count_by_field</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>